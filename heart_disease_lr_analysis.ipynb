{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c34e44c5",
   "metadata": {},
   "source": [
    "## Instalación de librerías necesarias\n",
    "\n",
    "En este cuaderno utilizaremos las siguientes librerías:\n",
    "\n",
    "- **NumPy**: Para operaciones matemáticas y manejo de arreglos.\n",
    "\n",
    "- **Pandas**: Para manipulación y análisis de datos tabulares.\n",
    "\n",
    "- **Matplotlib**: Para visualización de datos y gráficos.\n",
    "\n",
    "\n",
    "\n",
    "A continuación, instalamos las librerías necesarias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd814e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy pandas matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c39da89",
   "metadata": {},
   "source": [
    "## 1. Introducción\n",
    "\n",
    "La **enfermedad cardíaca** es una de las principales causas de mortalidad en el mundo. Su predicción temprana permite tomar medidas preventivas y mejorar la calidad de vida de los pacientes.\n",
    "\n",
    "\n",
    "\n",
    "La **regresión logística** es un modelo estadístico ampliamente utilizado para problemas de clasificación binaria, como la predicción de presencia o ausencia de enfermedad cardíaca.\n",
    "\n",
    "\n",
    "\n",
    "El dataset utilizado proviene de [Kaggle: neurocipher/heartdisease](https://www.kaggle.com/datasets/neurocipher/heartdisease) y se encuentra localmente en formato CSV.\n",
    "\n",
    "\n",
    "\n",
    "**Objetivo:** Implementar, analizar y evaluar un modelo de regresión logística para predecir enfermedad cardíaca usando únicamente NumPy, Pandas y Matplotlib, sin librerías externas de machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34df130c",
   "metadata": {},
   "source": [
    "## 2. Carga del dataset local\n",
    "\n",
    "Cargamos el archivo CSV usando pandas y exploramos su estructura."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e02601",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Cargar el dataset\n",
    "\n",
    "df = pd.read_csv(\"Heart_Disease_Prediction.csv\")\n",
    "\n",
    "# Mostrar primeros registros\n",
    "\n",
    "display(df.head())\n",
    "\n",
    "# Mostrar columnas\n",
    "print(\"Columnas:\", df.columns.tolist())\n",
    "\n",
    "# Mostrar dimensiones\n",
    "print(\"Dimensiones:\", df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4148d4",
   "metadata": {},
   "source": [
    "El dataset contiene las siguientes columnas y dimensiones. Se observa la estructura inicial para planificar el análisis posterior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c10a6a7",
   "metadata": {},
   "source": [
    "## 3. Análisis exploratorio de datos (EDA)\n",
    "\n",
    "Realizamos un análisis exploratorio para comprender la información y calidad del dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "babeb5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Información general\n",
    "df.info()\n",
    "\n",
    "# Estadísticas descriptivas\n",
    "display(df.describe())\n",
    "\n",
    "# Análisis de valores nulos\n",
    "print(\"Valores nulos por columna:\\n\", df.isnull().sum())\n",
    "\n",
    "# Detección de outliers usando boxplots\n",
    "num_cols = df.select_dtypes(include=[np.number]).columns\n",
    "plt.figure(figsize=(15,6))\n",
    "df[num_cols].boxplot()\n",
    "plt.title(\"Boxplots de variables numéricas\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "# Distribución del target\n",
    "target_col = 'Heart Disease' if 'Heart Disease' in df.columns else df.columns[-1]\n",
    "df[target_col].value_counts().plot(kind='bar', color=['tab:blue','tab:orange'])\n",
    "plt.title(\"Distribución de la variable objetivo\")\n",
    "plt.xlabel(\"Clase\")\n",
    "plt.ylabel(\"Cantidad\")\n",
    "plt.show()\n",
    "\n",
    "# Histogramas de variables relevantes\n",
    "plt.figure(figsize=(15,8))\n",
    "for i, col in enumerate(num_cols):\n",
    "    plt.subplot(2, int(np.ceil(len(num_cols)/2)), i+1)\n",
    "    df[col].hist(bins=20, color='tab:blue', alpha=0.7)\n",
    "    plt.title(col)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Matriz de correlación\n",
    "corr = df[num_cols].corr()\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.imshow(corr, cmap='coolwarm', interpolation='none')\n",
    "plt.colorbar()\n",
    "plt.xticks(range(len(corr)), corr.columns, rotation=90)\n",
    "plt.yticks(range(len(corr)), corr.columns)\n",
    "plt.title(\"Matriz de correlación\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47fb872c",
   "metadata": {},
   "source": [
    "Se observa la distribución de las variables, posibles outliers y la correlación entre variables. No se detectan valores nulos significativos. El target está balanceado/desbalanceado según el gráfico."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749dff34",
   "metadata": {},
   "source": [
    "## 4. Preprocesamiento de datos\n",
    "\n",
    "Preparamos los datos para el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78c10f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversión binaria del target si es necesario\n",
    "if df[target_col].nunique() > 2:\n",
    "    df[target_col] = (df[target_col] == df[target_col].max()).astype(int)\n",
    "\n",
    "# Selección de variables relevantes (ejemplo: edad, colesterol, presión, etc.)\n",
    "variables = ['Age', 'Sex', 'Chest Pain Type', 'Resting BP', 'Cholesterol', 'Fasting BS', 'Max HR']\n",
    "X = df[variables].values\n",
    "y = df[target_col].values\n",
    "\n",
    "# División estratificada 70/30\n",
    "def stratified_split(X, y, test_size=0.3, random_state=42):\n",
    "    np.random.seed(random_state)\n",
    "    idx_0 = np.where(y==0)[0]\n",
    "    idx_1 = np.where(y==1)[0]\n",
    "    n0, n1 = len(idx_0), len(idx_1)\n",
    "    n0_test, n1_test = int(n0*test_size), int(n1*test_size)\n",
    "    test_idx = np.concatenate([np.random.choice(idx_0, n0_test, replace=False),\n",
    "                              np.random.choice(idx_1, n1_test, replace=False)])\n",
    "    train_idx = np.setdiff1d(np.arange(len(y)), test_idx)\n",
    "    return X[train_idx], X[test_idx], y[train_idx], y[test_idx]\n",
    "\n",
    "X_train, X_test, y_train, y_test = stratified_split(X, y, test_size=0.3)\n",
    "\n",
    "# Normalización manual\n",
    "mean = X_train.mean(axis=0)\n",
    "std = X_train.std(axis=0)\n",
    "X_train_norm = (X_train - mean) / std\n",
    "X_test_norm = (X_test - mean) / std\n",
    "\n",
    "# Guardar parámetros\n",
    "norm_params = {'mean': mean, 'std': std}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d034542d",
   "metadata": {},
   "source": [
    "Se seleccionan variables relevantes basadas en el análisis previo. La división estratificada asegura representatividad. La normalización mejora la estabilidad numérica del modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4111074d",
   "metadata": {},
   "source": [
    "## 5. Implementación de Regresión Logística desde cero\n",
    "\n",
    "A continuación se implementan todas las funciones necesarias para el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1ced08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"Función sigmoide.\"\"\"\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def compute_cost(X, y, w, b):\n",
    "    \"\"\"Función de costo: entropía cruzada binaria.\"\"\"\n",
    "    m = X.shape[0]\n",
    "    z = X @ w + b\n",
    "    h = sigmoid(z)\n",
    "    eps = 1e-8\n",
    "    cost = -np.mean(y * np.log(h + eps) + (1 - y) * np.log(1 - h + eps))\n",
    "    return cost\n",
    "\n",
    "def compute_gradients(X, y, w, b):\n",
    "    \"\"\"Gradientes de la función de costo respecto a w y b.\"\"\"\n",
    "    m = X.shape[0]\n",
    "    z = X @ w + b\n",
    "    h = sigmoid(z)\n",
    "    error = h - y\n",
    "    dw = (1/m) * (X.T @ error)\n",
    "    db = (1/m) * np.sum(error)\n",
    "    return dw, db\n",
    "\n",
    "def gradient_descent(X, y, w_init, b_init, alpha, num_iters):\n",
    "    \"\"\"Entrenamiento por gradiente descendente.\"\"\"\n",
    "    w = w_init.copy()\n",
    "    b = b_init\n",
    "    cost_history = []\n",
    "    for i in range(num_iters):\n",
    "        dw, db = compute_gradients(X, y, w, b)\n",
    "        w -= alpha * dw\n",
    "        b -= alpha * db\n",
    "        cost = compute_cost(X, y, w, b)\n",
    "        cost_history.append(cost)\n",
    "    return w, b, cost_history\n",
    "\n",
    "def predict(X, w, b, threshold=0.5):\n",
    "    \"\"\"Predicción binaria.\"\"\"\n",
    "    probs = sigmoid(X @ w + b)\n",
    "    return (probs >= threshold).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb45f41",
   "metadata": {},
   "source": [
    "Las funciones implementan la lógica de la regresión logística, cálculo de costo, gradientes, entrenamiento y predicción."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70a5cb2",
   "metadata": {},
   "source": [
    "## 6. Entrenamiento del modelo\n",
    "\n",
    "Entrenamos el modelo y analizamos la convergencia del costo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96cf591f",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.01\n",
    "num_iters = 1500\n",
    "w_init = np.zeros(X_train_norm.shape[1])\n",
    "b_init = 0.0\n",
    "\n",
    "w, b, cost_history = gradient_descent(X_train_norm, y_train, w_init, b_init, alpha, num_iters)\n",
    "\n",
    "# Graficar costo vs iteraciones\n",
    "plt.plot(cost_history)\n",
    "plt.title(\"Costo vs Iteraciones\")\n",
    "plt.xlabel(\"Iteración\")\n",
    "plt.ylabel(\"Costo\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ceeb0a",
   "metadata": {},
   "source": [
    "El gráfico muestra la convergencia del costo durante el entrenamiento. Se observa una disminución progresiva, indicando aprendizaje efectivo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3110c6e",
   "metadata": {},
   "source": [
    "## 7. Evaluación del modelo\n",
    "\n",
    "Calculamos métricas de desempeño en train y test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c346c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_true, y_pred):\n",
    "    return np.mean(y_true == y_pred)\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    tp = np.sum((y_true==1) & (y_pred==1))\n",
    "    fp = np.sum((y_true==0) & (y_pred==1))\n",
    "    return tp / (tp + fp + 1e-8)\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "    tp = np.sum((y_true==1) & (y_pred==1))\n",
    "    fn = np.sum((y_true==1) & (y_pred==0))\n",
    "    return tp / (tp + fn + 1e-8)\n",
    "\n",
    "def f1_score(y_true, y_pred):\n",
    "    p = precision(y_true, y_pred)\n",
    "    r = recall(y_true, y_pred)\n",
    "    return 2 * p * r / (p + r + 1e-8)\n",
    "\n",
    "# Predicciones\n",
    "y_train_pred = predict(X_train_norm, w, b)\n",
    "y_test_pred = predict(X_test_norm, w, b)\n",
    "\n",
    "# Métricas\n",
    "metrics = {\n",
    "    'Conjunto': ['Entrenamiento', 'Test'],\n",
    "    'Accuracy': [accuracy(y_train, y_train_pred), accuracy(y_test, y_test_pred)],\n",
    "    'Precision': [precision(y_train, y_train_pred), precision(y_test, y_test_pred)],\n",
    "    'Recall': [recall(y_train, y_train_pred), recall(y_test, y_test_pred)],\n",
    "    'F1-score': [f1_score(y_train, y_train_pred), f1_score(y_test, y_test_pred)]\n",
    "}\n",
    "import pandas as pd\n",
    "display(pd.DataFrame(metrics))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac012b0",
   "metadata": {},
   "source": [
    "La tabla muestra el desempeño del modelo en ambos conjuntos. Se observa la capacidad de generalización y posibles áreas de mejora."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac1956e",
   "metadata": {},
   "source": [
    "## 8. Fronteras de decisión en 2D\n",
    "\n",
    "Analizamos la separabilidad usando pares de variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350b572f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_boundary_2d(X, y, w, b, var_names):\n",
    "    plt.figure(figsize=(7,6))\n",
    "    plt.scatter(X[y==0,0], X[y==0,1], label='No Enfermedad', alpha=0.7)\n",
    "    plt.scatter(X[y==1,0], X[y==1,1], label='Enfermedad', alpha=0.7)\n",
    "    x_min, x_max = X[:,0].min()-1, X[:,0].max()+1\n",
    "    y_min, y_max = X[:,1].min()-1, X[:,1].max()+1\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100), np.linspace(y_min, y_max, 100))\n",
    "    grid = np.c_[xx.ravel(), yy.ravel()]\n",
    "    z = grid @ w + b\n",
    "    probs = sigmoid(z).reshape(xx.shape)\n",
    "    plt.contour(xx, yy, probs, levels=[0.5], colors='k')\n",
    "    plt.xlabel(var_names[0])\n",
    "    plt.ylabel(var_names[1])\n",
    "    plt.title(f'Frontera de decisión: {var_names[0]} vs {var_names[1]}')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "pares = [(0,1), (2,3), (4,5)]\n",
    "for i,j in pares:\n",
    "    X2_train = X_train_norm[:,[i,j]]\n",
    "    w2_init = np.zeros(2)\n",
    "    b2_init = 0.0\n",
    "    w2, b2, _ = gradient_descent(X2_train, y_train, w2_init, b2_init, alpha, num_iters)\n",
    "    plot_decision_boundary_2d(X2_train, y_train, w2, b2, [variables[i], variables[j]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef21e4f",
   "metadata": {},
   "source": [
    "Se observa la frontera de decisión para diferentes pares de variables. El grado de separabilidad varía según las características seleccionadas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e003fb",
   "metadata": {},
   "source": [
    "## 9. Regularización L2\n",
    "\n",
    "Agregamos regularización L2 al modelo y analizamos su efecto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c956320c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost_reg(X, y, w, b, lam):\n",
    "    m = X.shape[0]\n",
    "    z = X @ w + b\n",
    "    h = sigmoid(z)\n",
    "    eps = 1e-8\n",
    "    cost = -np.mean(y * np.log(h + eps) + (1 - y) * np.log(1 - h + eps))\n",
    "    reg = (lam/(2*m)) * np.sum(w**2)\n",
    "    return cost + reg\n",
    "\n",
    "def compute_gradients_reg(X, y, w, b, lam):\n",
    "    m = X.shape[0]\n",
    "    z = X @ w + b\n",
    "    h = sigmoid(z)\n",
    "    error = h - y\n",
    "    dw = (1/m) * (X.T @ error) + (lam/m) * w\n",
    "    db = (1/m) * np.sum(error)\n",
    "    return dw, db\n",
    "\n",
    "def gradient_descent_reg(X, y, w_init, b_init, alpha, num_iters, lam):\n",
    "    w = w_init.copy()\n",
    "    b = b_init\n",
    "    cost_history = []\n",
    "    for i in range(num_iters):\n",
    "        dw, db = compute_gradients_reg(X, y, w, b, lam)\n",
    "        w -= alpha * dw\n",
    "        b -= alpha * db\n",
    "        cost = compute_cost_reg(X, y, w, b, lam)\n",
    "        cost_history.append(cost)\n",
    "    return w, b, cost_history\n",
    "\n",
    "lambdas = [0, 0.001, 0.01, 0.1, 1]\n",
    "results = []\n",
    "for lam in lambdas:\n",
    "    w_r, b_r, cost_hist_r = gradient_descent_reg(X_train_norm, y_train, w_init, b_init, alpha, num_iters, lam)\n",
    "    y_test_pred_r = predict(X_test_norm, w_r, b_r)\n",
    "    acc = accuracy(y_test, y_test_pred_r)\n",
    "    norm_w = np.linalg.norm(w_r)\n",
    "    results.append({'lambda': lam, 'accuracy': acc, 'norm_w': norm_w, 'cost': cost_hist_r[-1], 'w': w_r, 'b': b_r})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286707b7",
   "metadata": {},
   "source": [
    "Se observa cómo la regularización afecta los pesos y el desempeño del modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a65e52",
   "metadata": {},
   "source": [
    "## 10. Análisis de regularización\n",
    "\n",
    "Analizamos el impacto de lambda en la precisión y la norma de los pesos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be3cb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lambdas_plot = [r['lambda'] for r in results]\n",
    "accs_plot = [r['accuracy'] for r in results]\n",
    "norms_plot = [r['norm_w'] for r in results]\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(lambdas_plot, accs_plot, marker='o')\n",
    "plt.title(\"Lambda vs Accuracy\")\n",
    "plt.xlabel(\"Lambda\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(lambdas_plot, norms_plot, marker='o', color='tab:orange')\n",
    "plt.title(\"Lambda vs Norma de w\")\n",
    "plt.xlabel(\"Lambda\")\n",
    "plt.ylabel(\"Norma de w\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Comparar fronteras para lambda bajo y alto\n",
    "for lam in [0, 1]:\n",
    "    idx = lambdas_plot.index(lam)\n",
    "    w_r = results[idx]['w']\n",
    "    b_r = results[idx]['b']\n",
    "    plt.figure(figsize=(7,6))\n",
    "    plt.scatter(X_test_norm[y_test==0,0], X_test_norm[y_test==0,1], label='No Enfermedad', alpha=0.7)\n",
    "    plt.scatter(X_test_norm[y_test==1,0], X_test_norm[y_test==1,1], label='Enfermedad', alpha=0.7)\n",
    "    x_min, x_max = X_test_norm[:,0].min()-1, X_test_norm[:,0].max()+1\n",
    "    y_min, y_max = X_test_norm[:,1].min()-1, X_test_norm[:,1].max()+1\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100), np.linspace(y_min, y_max, 100))\n",
    "    grid = np.c_[xx.ravel(), yy.ravel()]\n",
    "    z = grid @ w_r[:2] + b_r\n",
    "    probs = sigmoid(z).reshape(xx.shape)\n",
    "    plt.contour(xx, yy, probs, levels=[0.5], colors='k')\n",
    "    plt.xlabel(variables[0])\n",
    "    plt.ylabel(variables[1])\n",
    "    plt.title(f'Frontera lambda={lam}')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97617896",
   "metadata": {},
   "source": [
    "La regularización reduce la norma de los pesos y puede mejorar la generalización, evitando el sobreajuste. Fronteras más suaves corresponden a mayor lambda."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe50dd6",
   "metadata": {},
   "source": [
    "## 11. Exportación del modelo\n",
    "\n",
    "Guardamos los parámetros entrenados para uso futuro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64b524f",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"pesos.npy\", w)\n",
    "np.save(\"bias.npy\", b)\n",
    "print(\"Pesos y bias guardados exitosamente.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca2a39a",
   "metadata": {},
   "source": [
    "Los archivos `pesos.npy` y `bias.npy` permiten cargar el modelo y realizar inferencias en nuevos datos normalizados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ca8d9c",
   "metadata": {},
   "source": [
    "## 12. Despliegue en Amazon SageMaker\n",
    "\n",
    "Para desplegar el modelo en Amazon SageMaker se deben seguir los siguientes pasos:\n",
    "\n",
    "1. Crear una instancia de notebook en SageMaker.\n",
    "\n",
    "2. Subir el cuaderno y el archivo CSV al entorno.\n",
    "\n",
    "3. Adaptar el script de entrenamiento para ejecutarse en SageMaker.\n",
    "\n",
    "4. Crear un endpoint para servir el modelo.\n",
    "\n",
    "5. Realizar inferencias enviando datos al endpoint.\n",
    "\n",
    "6. Probar el modelo con ejemplos (por ejemplo: Edad=60, Colesterol=300 → Probabilidad=0.68).\n",
    "\n",
    "7. Monitorear el desempeño y actualizar el modelo según sea necesario.\n",
    "\n",
    "\n",
    "\n",
    "No se requieren credenciales reales para este ejemplo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f840203",
   "metadata": {},
   "source": [
    "## 13. Conclusiones finales\n",
    "\n",
    "- Se implementó un modelo de regresión logística desde cero para predecir enfermedad cardíaca.\n",
    "\n",
    "- El mejor desempeño se obtuvo con lambda óptimo según las métricas.\n",
    "\n",
    "- La regularización L2 ayuda a evitar el sobreajuste y mejora la generalización.\n",
    "\n",
    "- El modelo es interpretable y eficiente, aunque limitado frente a modelos más complejos.\n",
    "\n",
    "- Futuras mejoras incluyen ingeniería de variables, validación cruzada y comparación con otros algoritmos."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
